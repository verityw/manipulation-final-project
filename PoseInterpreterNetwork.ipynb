{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoseInterpreterNetwork",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/verityw/manipulation-final-project/blob/main/PoseInterpreterNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h66azi4JexoO",
        "outputId": "5eae8bf4-2a53-4ab0-8c47-cfecc358e1ce"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models\n",
        "import collections\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "SAMPLES = -1\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 5\n",
        "IMG_SHAPE = (480, 640, 4)\n",
        "POSE_SHAPE = (6,)\n",
        "VAL_PROPORTION = .1\n",
        "device = \"cuda\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Bbsm2MfIlo"
      },
      "source": [
        "#cd to directory containing this notebook"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBSzf2elfLEh"
      },
      "source": [
        "DATA_DIRECTORY = os.path.join(os.getcwd(), \"data\")\n",
        "X_DIR, Y_DIR = os.path.join(DATA_DIRECTORY, \"x\"), os.path.join(DATA_DIRECTORY, \"y\")\n",
        "XVAL_DIR, YVAL_DIR = os.path.join(DATA_DIRECTORY, \"xval\"), os.path.join(DATA_DIRECTORY, \"yval\")\n",
        "CHECKPOINTS = os.path.join(os.getcwd(), \"checkpoints\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Xmf0IaJfHC"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x_path, y_path, num_items):\n",
        "        self.x_path = x_path\n",
        "        self.y_path = y_path\n",
        "        self.indices = np.arange(num_items, dtype=int)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        ind = self.indices[index]\n",
        "        img = np.load(os.path.join(self.x_path, str(index) + \"img.npy\"))\n",
        "        img = torch.tensor(np.transpose(img, (2, 0, 1))) / 255.\n",
        "        pose = torch.tensor(np.load(os.path.join(os.path.join(self.y_path, str(index) + \"pose.npy\"))))\n",
        "        pos, rot = pose[:3], pose[3:]\n",
        "        rot = rot * np.sign(rot[0]) # Make sure qw > 0\n",
        "        return (img, [pos, rot]) # Return 3 x H x W image, (x, y, z), and (qw, qx, qy, qz)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbabJgmAHuCK"
      },
      "source": [
        "def conv(in_channels, out_channels, kernel_size):\n",
        "    padding = (kernel_size-1) // 2\n",
        "    assert 2*padding == kernel_size-1, \"parameters incorrect. kernel={}, padding={}\".format(kernel_size, padding)\n",
        "    return nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=padding,bias=False),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "def fc(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_channels, out_channels),\n",
        "    )\n",
        "\n",
        "class normalize(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(normalize, self).__init__()\n",
        "    def forward(self, x):\n",
        "        norms = x.norm(dim=1, keepdim=True)\n",
        "        x = x.div(norms)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoseEstimatorNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoseEstimatorNet, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet18()\n",
        "        self.initial = conv(4, 3, 1) # in_channels = 4 for RGBAlpha\n",
        "        self.fc1 = fc(1000, 256)\n",
        "        self.fc2_position = fc(256, 3)\n",
        "        self.fc2_rotation = fc(256, 4)\n",
        "        self.normalize = normalize()\n",
        "        self.relu = torch.nn.ReLU() \n",
        "    def forward(self, x):\n",
        "        # Convolve to be acceptable size for resnet\n",
        "        x = self.initial(x)\n",
        "        # ResNet18\n",
        "        x = self.resnet(x)\n",
        "        # Multi-layer perceptron\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x_pos = self.fc2_position(x)\n",
        "        x_rot = self.fc2_rotation(x)\n",
        "        x_rot = self.normalize(x_rot)\n",
        "\n",
        "        return x_pos, x_rot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1x3-_Du0YmV"
      },
      "source": [
        "# Losses\n",
        "\n",
        "# L1 loss\n",
        "class L1Loss(nn.Module):\n",
        "    def __init__(self, alpha = 1):\n",
        "        super(L1Loss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, y_pred, y_target):\n",
        "        return torch.mean(torch.abs(y_pred[0] - y_target[0])) + self.alpha * torch.mean(torch.abs(y_pred[1] - y_target[1])) \n",
        "\n",
        "# PoseCNN loss\n",
        "class PoseCNNLoss(nn.Module):\n",
        "    def __init__(self, alpha = 1):\n",
        "        super(PoseCNNLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, y_pred, y_target):\n",
        "        position_loss = torch.abs(y_target[0] - y_pred[0]).mean()\n",
        "        orientation_loss = (1 - (y_pred[1] * y_target[1]).sum(dim=1).pow(2)).mean()\n",
        "        return position_loss + self.alpha * orientation_loss + torch.maximum(torch.tensor([0], device=device), -y_pred[0]).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIwb5fEVrA0v"
      },
      "source": [
        "# Train the network\n",
        "def iterate(mode, loader, model, optimizer, criterion, epoch):\n",
        "    print(\"Starting epoch: \" + str(epoch))\n",
        "    if mode == \"train\":\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    running_loss = 0\n",
        "    for i, [img, label] in enumerate(loader):\n",
        "        img = img.to(device)\n",
        "        pos, rot = label[0], label[1]\n",
        "        pos, rot = pos.to(device), rot.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        pos_pred, rot_pred = model(img)\n",
        "        \n",
        "        # Backpropagation\n",
        "        loss = criterion([pos_pred, rot_pred], [pos, rot])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss\n",
        "        if i % 100 == 0:\n",
        "            print(\"Batch: \" + str(i+1))\n",
        "            print(\"Batch Loss: \" + str(loss))\n",
        "            print(\"Average Loss:\" + str(running_loss / (i + 1)))\n",
        "    print(\"Finished epoch.\")\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "print(\"Initializing model\")\n",
        "model = PoseEstimatorNet()\n",
        "model.to(device)\n",
        "print(\"Initializing optimizer\")\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "print(\"Initializing loss criterion\")\n",
        "criterion = PoseCNNLoss(alpha=.1)\n",
        "print(\"Initializing dataloaders\")\n",
        "train_loader = torch.utils.data.DataLoader(Dataset(X_DIR, Y_DIR, 10000), batch_size = 10)\n",
        "#val_loader = torch.utils.data.DataLoader(Dataset(XVAL_DIR, YVAL_DIR, 2000), batch_size = 10)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    loss = iterate(\"train\", train_loader, model, optimizer, criterion, epoch)\n",
        "    #val_loss = iterate(\"val\", train_loader, model, optimizer, criterion, epoch)\n",
        "    torch.save(model.state_dict(), os.path.join(CHECKPOINTS, str(loss) + \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAhvPJr43wBH",
        "outputId": "3c4b46e7-860b-4d80-ca38-bbc27a1a2e35"
      },
      "source": [
        "Model = PoseEstimatorNet()\n",
        "Model.load_state_dict(torch.load(os.path.join(CHECKPOINTS, \"tensor(0.0926, device='cuda:0', dtype=torch.float64, grad_fn=<DivBackward0>)\"), map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwMFrPm20zDx"
      },
      "source": [
        "val_loader = torch.utils.data.DataLoader(Dataset(XVAL_DIR, YVAL_DIR, 2000), batch_size = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ony6hcLo1CqQ"
      },
      "source": [
        "img, label = next(iter(val_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi6utCCB1GBe"
      },
      "source": [
        "pos, rot = Model(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MBA7tYl1UNk",
        "outputId": "c8a96aa9-cb0e-4ec2-c3f0-c814efa6439f"
      },
      "source": [
        "print(pos)\n",
        "print(rot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0377, 0.0474, 0.1031]], grad_fn=<AddmmBackward>)\n",
            "tensor([[-0.6211, -0.6342, -0.4091, -0.2114]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHJpWxkZ1YlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0e131e-c1b0-4242-ad36-265aede8e51c"
      },
      "source": [
        "print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[ 0.0486, -0.0219,  0.0468]], dtype=torch.float64), tensor([[ 0.6148, -0.0229, -0.7881, -0.0180]], dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v1lTicp1ePB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}